# Data Management

Admittedly, I have said the same thing about @sec-git on version control, yet *data management* may be the single most impactful chapter of this book. This may be the case in particular in case you came from an environment that mostly organized data in spreadsheets shared via e-mail or network drives. To contextualize data and think about (long term) data management is a step into a bigger world. 

After decades of information engineering and computer science, some can't help but wonder why we have not found one perfect, one-size-fits-all form of data. In fact, a substantial part of programming with data deals with transforming data from one form into another. This chapter intends to give an idea of the aspects of data management most relevant for data analytics. Hopefully this chapters helps the reader to assess how far they wanted to dig into data management. 

## Forms of Data 

In research and analytics, data appear in a plethora of different forms. Yet, most researchers and business analysts are mostly trained to handle different flavors of two dimensional data as in **one-observation-one-row**. Adhoc studies conducted once result in *cross sectional data*: one line per observation, columns represent variables. Sensor data, server logs or forecasts of official statistics are examples of single variable data observed over time. These single variable, longitudinal data are also known as *time series*. Multivariate time series, i.e., multi-variable, longitudinal data are often referred to as *panel data*. In our heads, all of these forms of data are typically represented as rectangular, two-dimensional one-line-per-observation, spreadsheet like tables. Here are a few easy-to-reproduce examples using popular R demo datasets. 

```{r}
#| label: moo
#| lst-cap: 'moo'
#| echo: true
h <- head(mtcars)
h
dim(h)
```

The above output shows an excerpt of the *mtcars* **cross-sectional** dataset with 6 lines and 11 variables.  *Airpassenger*
is a **time series** dataset represented in an R *ts* object which is essentially a vector with time based index attribute.    

```{r}
AirPassengers
```


Let's create **multivariate time series (panel)**  dataset, i.e., multiple variables observed over time: 

```{r}
d <- data.frame(Var1 = rnorm(10, 0),
           Var2 = rnorm(10, 10),
           Var3 = rnorm(10, 30))
multi_ts <- ts(d, start = c(2000,1), frequency = 4)
multi_ts
```

 
**A Note on Long Format vs. Wide Format**

> The above multi-variable time series is shown in what the data science community calls *wide* format. In this most intuitive format, every column represents one variable, time is on the Y-axis. The counterpart is the so called *long* format shown below. The long format is a machine friendly, flexible way to represent multi-variable data without altering the number of columns with more variables. 

```{r, message=FALSE}
library(tsbox)
ts_dt(multi_ts)[1:15,]

```

>The ability to transform data from one format into the other and to manipulate both formats is an essential skill for any data scientist or data engineer. It is important to point out that the ability to do the above transformations effortlessly is an absolute go-to skill for people who want to use programming to run analysis. (Different analyses or visualizations may require one form or the other and ask for quick transformation).

Hence popular data science programming languages offer great toolsets to get the job done. Mastering these toolboxes is not the focus of this book. R for Data Science and the Carpentries are good starting points if you feel the need to catch up or solidify your know-how. 

Yet, not all information suits a two dimensional form. Handling nested or unstructured information is one of the fields where the strength of a programming approach to data analysis and visualization comes into play. Maps are a common form of information that is often represented in nested fashion. For an example of nested data, let's take a look at the map file and code example case study in @sec-map. In memory, i.e., in our R session the data is represented in a list that contains multiple list may contain more lists nested inside. 

```{r}
library(jsonlite)

json_ch <- jsonlite::read_json(
  "https://raw.githubusercontent.com/mbannert/maps/master/ch_bfs_regions.geojson"
)
ls.str(json_ch)
```

Another example of nested but structured data are HTML or XML trees obtained from scraping websites. Typically, web scraping approaches like [rvest]() or [beautiful soup]() parse the hierarchical Document Object Model (DOM) and turn it into an in-memory representation of a website's DOM. For DOM parsing example see CASE STUDY XYZ. 


## Representing Data in Files

To create the above examples of different forms of data, it was mostly sufficient to represent data in memory, in this case within an R session. As an interpreted language, an R interpreter has to run at all times when using R. The very same is true for Python. Users of these language can work interactively, very much like with a pocket calculator on heavy steroids. All functions, all data, are in loaded into a machine's RAM (memory) represented as objects of various classes. This is convenient, but has an obvious limitation: once the sessions ends, the information is gone. Hence we need to have a way to store at least the results of our computation in persistent fashion. 

Just like in office or image editing software, the intuitive way to store data persistently from a programming language is to store data into files. The choice of the file format is much less straight forward in our case though. The different forms of data discussed above, potential collaborators and interfaces are factors among others than weigh into our choice of a file format. 

### Spreadsheets

Based on our two dimension focused intuition and training spreadsheets are the on-disk analog of data.frames, data.tables and tibbles. Formats like **.csv** or **.xlsx** are the most common way to represent two dimensional data on disk.  
On the programming side the ubiquity of spreadsheets leads to a wide variety of libraries to parse and write different spreadsheet formats. 

```{python}
import csv
import pandas as pd

d = {'column1': [1,2], 'column2': [3,4]}
df = pd.DataFrame(data=d)
df.to_csv("an_example.csv", sep=";", encoding='utf-8')

```

Comma separated values (.csv)^[Note that commas are not always necessarily the seperator in .csv files. Because of the use of commas as decimal delimiters in some regions, columns are also often separated by semicolons to avoid conflicts.] are good and simple option. Their text based nature makes .csv files language agnostic and human readable through a text editor. 

```
;column1;column2
0;1;3
1;2;4
```

Though Excel spreadsheets are a convenient interface to office environments that offer extras such organization into workbooks, the simpler .csv format has advatanges in machine-to-machine communication and as an interface between different programming languages and tools. For example, web visualization libraries such as highcharts or echarts are most commonly written in javascript and can conveniently consume data from .csv files. The above example .csv file was written by Python and is now easily read by R. 

```{r}
library(readr)
csv <- readr::read_csv2("an_example.csv")
csv

```

### File Formats for Nested Information

For many data engineers and developers. [Javascript Object Notation (JSON)](https://json.org) has become the go-to file format for nested data. Just like with .csv basically every programming language used in data science and analytics has libraries to serialize and de-serialize JSON (read and write). Though harder to read for humans than .csv, pretty-fied JSON with a decent highlighting color scheme is easy to read and gives the human reader a good understanding of the hierarchy at hand. The added complexity comes mostly from the nested nature of the data, not so much from the file format. 


```{r}
library(jsonlite)

li <- list(
  element_1 = head(mtcars, 2),
  element_2 = head(iris, 2)
)

toJSON(li, pretty = TRUE)

```

The above example shows the first two lines of two different, unrelated rectangular datasets. Thanks to the hierarchical nature of JSON both datasets can be stored in the same file albeit totally different columns. Again, just like .csv, JSON works well as an interface, but is more flexible than the former. 

Besides JSON, **XML** is the most common format to represent nested data in files. Though there are a lot of overlapping use cases, there is a bit of a different groove around both of these file formats. JSON is perceived as more light weight and close to 'the web' while XML is the traditional, very explicit no-nonsense format.  XML has a **Document Type Defintion (DTD)** that defines the structure of the document and which elements and attributes are legal. Higher level formats use this more formal approach for as XML based defintion. [SDMX](https://sdmx.org/)^[SDMX.org about SDMX: SDMX, which stands for Statistical Data and Metadata eXchange is an international initiative that aims at standardising and modernising (“industrialising”) the mechanisms and processes for the exchange of statistical data and metadata among international organisations and their member countries. SDMX is sponsored by seven international organisations including the Bank for International Settlements (BIS), the European Central Bank (ECB), Eurostat (Statistical Office of the European Union), the International Monetary Fund (IMF), the Organisation for Economic Cooperation and Development (OECD), the United Nations Statistical Division (UNSD), and the World Bank], a world-wide effort to provide a format for exchange statistical data and meta data, is an example of such a higher level format build on XML. 


```{xml}
<CompactData xmlns:c="http://www.SDMX.org/resources/SDMXML/schemas/v2_0/compact" xmlns:sdds="urn:sdmx:org.sdmx.infomodel.datastructure.DataStructure=IMF:ECOFIN_DSD" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:msg="http://www.SDMX.org/resources/SDMXML/schemas/v2_0/message">
<Header>
<ID>kofbarometer</ID>
<Test>false</Test>
<Prepared>2022-07-27 23:19:15</Prepared>
<Sender id="KOF">
<Name xml:lang="en">KOF Swiss Economic Institute</Name>
<Contact>
<Name>KOF Swiss Economic Institute</Name>
<URI>http://www.kof.ethz.ch/prognosen-indikatoren/indikatoren/kof-konjunkturbarometer.html</URI>
</Contact>
</Sender>
</Header>
<sdds:DataSet>
<sdds:Series DATA_DOMAIN="FLI" REF_AREA="CH" INDICATOR="AL" COUNTERPART_AREA="_Z" FREQ="M" UNIT_MULT="0" TIME_FORMAT="P3M">
<sdds:Obs TIME_PERIOD="1991-01" OBS_VALUE="79.7534465342489" OBS_STATUS="A"/>
<sdds:Obs TIME_PERIOD="1991-02" OBS_VALUE="71.8659035408967" OBS_STATUS="A"/>

....

<sdds:Obs TIME_PERIOD="2022-05" OBS_VALUE="96.4419659191275" OBS_STATUS="A"/>
<sdds:Obs TIME_PERIOD="2022-06" OBS_VALUE="95.1748194208808" OBS_STATUS="A"/>
<sdds:Obs TIME_PERIOD="2022-07" OBS_VALUE="90.08821804515" OBS_STATUS="A"/>
</sdds:Series>
</sdds:DataSet>
</CompactData>
<!--  i: 1403 : 1660204311  -->

```

The above example shows and excerpt of the main economic forward looking indicator (FLI) for Switzerland, the KOF Economic Barometer, represented in an SDMX XML file. Besides the value and the date index, several attributes provide the consumer with an elaborate data description. In addition, other nodes and their children provide information like *Contact* or *ID* in the very same file. Note that modern browser often provide code folding for nodes and highlighting to improve readability.  


### A Word on Binaries

Unlike all file formats discussed above, binaries cannot be read by humans using a simple text editor. In other words, you will need the software that wrote the binary to read it again. If that software was expensive and/or exotic, your work is much less accessible, more difficult to share and harder to reproduce. Though this disadvantage of binaries is mitigated when you use freely available open source software, storing data in binaries can still be a hurdle. 

But of course binaries do have advantages, too: binaries can compress their content and save space. Binaries can take on all sorts of in-memory objects including functions, not just datasets. In other words, binaries can bundle stuff. Consider the following load/save operation in R: 

```{r, eval=FALSE}
bogus <- function(a,b){
  a + b
} 

data(Airpassengers)
data(mtcars)

s <- summary(mtcars)

save("bogus", "Airpassengers","s", file="bundle.RData")

```

In memory, *bogus* is a *function*, Airpassengers is an R *time series* object and *s* is a *list* based summary object. All of these objects can be stored in a single binary RData file using *save()*. A fresh R session can now *load()* everything stored in that file. 


```{r, eval=FALSE}
load("bundle.RData")
```

>Notice that unlike reading a .csv or .json file, the call does not make any assignments into a target object. This is because all objects are loaded into an R environment (*.globalEnv* by default) with their original names.  


### Interoperable File Formats

Interoperable file formats cover some middle ground between the options described above. The *cross-language in-memory development platform* [Apache Arrow](https://arrow.apache.org/) is a well established project that also implements file formats that work across many popular (data science) environments. Though the major contribution of the Apache Arrow project is to allow to share in-memory data store across environments, I will just show it as an example for interoperable file formats here. Nevertheless if you're interested in a modern, yet established cross environment data science project, diggin deeper into Apache Arrow is certainly a fun experience. 

From the Apache Arrow documentation:

```{r, message=FALSE}
library(dplyr)
library(arrow)
data("starwars")
file_path_sw <- "starwars.parquet"
write_parquet(starwars, file_path_sw)

```

The above R code writes the *starwars* demo dataset from the *dplyr* R package to a temporary .parquet file. 
The arrow R packages comes with the necessary toolset to write the open source columnar^[see also] .parquet format. Though they are not text files, .parquet files can be read and written from different environments and consume the file written with R. The below code uses the arrow library for Python to read the file we have just written with R. 

```{python}
import pyarrow.parquet as pa
pa.read_table("starwars.parquet")
```

Here's Julia reading our Parquet file: 

```{julia}
using Parquet
sw = Parquet.File("starwars.parquet")
sw.schema.schema
```

>When I composed this example reading and writing parquet files in different environments I ran into several compatibility issues. This shows that the level of interoperability is not at the same level as the interoperability of text files. 


#### A Note on Overhead {.unnumbered}{#columnar}

The *parquet* format is designed to read and write files swiftly and to consume less disk space than text files. Both features can become particularly relevant in the cloud. Note though that parquet comes with some overhead which may eat up gains if datasets are small. Consider our *starwars* dataset. At 87 rows and 14 columns the dataset is rather small.

```{r}
library(readr)
write_csv(starwars, file = "starwars.csv")
dim(starwars)
round(file.size("starwars.parquet") / file.size("starwars.csv"), digits = 2)
    
```

Hence the overhead of a schema implementation and other meta information outweighs parquet's compression for such a small dataset leading to a parquet file that is almost 1.5 times larger than the corresponding csv file. Yet, parquet already turns the tables for the *diamonds* demo dataset from the *ggplot2* R package which is by no means a large dataset. 

```{r}
library(ggplot2)
data(diamonds)
write_csv(diamonds, file = "diamonds.csv")
write_parquet(diamonds, "diamonds.parquet" )
round(file.size("diamonds.parquet") / file.size("diamonds.csv"), digits = 2)

```

The parquet file for the diamonds dataset has roughly one fifth of the size of the corresponding text file. 
This is a great example of why there is not one single, perfect, one-size-fits all form of data that emerged from
decades of information engineering. So when you choose how you are going to represent data in our project, think about your goals, your most common use or query and a smooth data transformation strategy for when the use cases or goals change. 


## Databases

Given the options that file based approaches provide, what is a) the difference and b) the added value of going for database to manage data? The front-and-center difference is the client interface, but there are many more differences and benefits.

![PostgreSQL client]()

The above figure shows a terminal client which users use to send queries written in a query language. The client sends these queries to a database host and either performs an operation on the database quietly or returns a result. The most common example of such a query language is the Structured Query Language (SQL). This leads to a standard way of interaction with the data no matter how the dataset looks like in terms of dimensions, size etc. SQL databases have been around much longer than data science itself and continue to be inevitable as application backends and data archives for many use cases. 


```{sql, eval=FALSE}
SELECT * FROM myschema.demotable
```

The above query would return all rows and all columns from a table called *demotable* in a schema called *myschema*. Such a query can easier be send from a standalone database client, a database specific IDE with a built in client such as [DataGrid]() or a programming language. Given the ubiquity of database most basically any programming language has native interfaces to the most common database. And if that is not the case there is the database management system agnostic ODBC standard that is supported by all major SQL database. The below code shows how to connect from R to PostgreSQL, send queries from within R and receive results as R objects. 

```{r,eval=FALSE}
library(RPostgres)
con <- dbConnect(
  host = "localhost",
  user = "bugsbunny",
  passwd = .rs.AskForPassword("Enter Pw"), # only works within RStudio,
  dbname = "some_db_name"
)

# the result is an R data.frame
res <- dbSendQuery(con, "SELECT * FROM myschema.demotable")

# and we can do R things with it
# such as show the first 6 lines.
head(res)
dbDisconnect(con)
```

Obviously the above example barely shows the tip of the iceberg as it is just meant to illustrate the way we interact with databases as opposed to a file system. To dig a little deeper into databases, I recommend to get a solid understanding of the basic CREATE, SELECT, INSERT, UPDATE, DELETE, TRUNCATE, DROP processes as well as basic JOINs and WHERE clauses. Also, it is helpful to understand the concept of normalization up to the third normal form. 


### Relational Database Management Systems (RDBMS)

When you need to pick a concrete database technology for your project, the first major choice is whether to go for a relational system or not. Unless you have a very compelling reason not to, you are almost always better off with a relational database: Relational databases are well established and accessible from any programming language used in programming with data that one could think of. In addition, modern RDBMS implmentations offer many non-relational features such as JSON field types and operations. 

I would classify the most popular relational database implementations as follows. First, there is [SQLite](https://www.sqlite.org/index.html). As the name suggestions, *SQLite* is a light-weight, stripped down, easy-to-use and install implementation. 

>SQLite is a C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. SQLite is the most used database engine in the world. SQLite is built into all mobile phones and most computers and comes bundled inside countless other applications that people use every day. 
-- SQLite.org

*SQLite* data lives in a single file that the user queries through the *SQLite* engine. Here is an example using that
engine from R. 

```{r}
library(RSQLite)
db_path <- "devopscarpentry.sqlite3"
con <- dbConnect(RSQLite::SQLite(), db_path)
dbWriteTable(con, dbQuoteIdentifier(con,"mtcars"), mtcars, overwrite = T)
dbWriteTable(con, dbQuoteIdentifier(con,"flowers"), iris, overwrite = T)
```

The above code initiates a *SQLite* database and continues to write the built-in R demo datasets into separate tables in that newly created database. Now we can use SQL to query the data. Return the first three rows of *flowers*:

```{r}
dbGetQuery(con, "SELECT * FROM flowers LIMIT 3")
```

Return cars that are more fuel efficient than 30 miles per gallon:

```{r}
dbGetQuery(con, "SELECT * FROM mtcars WHERE mpg > 30")
```

[MySQL](https://www.mysql.com/) can do a little more and also immensely popular, particularly as a database backend for web content management systems and other web based applications. The so-called LAMP stack (Linux, Apache, MySQL and PHP) contributed to its rise decades ago when it fueled many smaller and medium level web projects around the world. In its early days MySQL used to be an independent open source project, but was later on acquired by database juggernaught Oracle as a lite version to go with its flagship product. 

While certainly doing its job in millions of installation, MySQL it is not in at same level as [Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-2019) (MSSQL), 
[PostgreSQL](https://www.postgresql.org/) and [Oracle Database](https://www.oracle.com/database/technologies/) and I suggest one of these three enterprise level database as data store for a research projects that go beyond hosting a blog. Especially when it comes to long term conservation of data and enforcing consistency MSSQL, PostgreSQL and Oracle are hard to beat. 
Among the three, personally I would always lean towards the license cost free open source PostgreSQL, but fitting into existing ecosystems is a good reason to go with either of MSSQL or Oracle if you can afford the licenses. For many use cases, there is hardly any difference for the analyst or scientific enduser. PostgreSQL may have the coolest spatial support, MSSQL T-SQL dialect may have some extra convenient queries if your developers mastered the dialect and Oracle may have the edge in performance and Java interaction here and there, but none of these systems is a bad choice. 

Another database management that gets a lot of attention recently (and rightfully so) is [DuckDB](https://duckdb.org/). Because it is mentioned so overwhelmingly positive and often, it is important to understand what it is and when to use it. DuckDB is not yet another competitor that tries to gain some ground from the big three of MSSQL, PostgreSQL and Oracle. DuckDB does offers an SQL interface but is very different in its aims from the traditional SQL databases. DuckDB is serverless and allows to access parquet files via a very fast SQL interface. This makes DuckDB a great tool for interactive analysis and transfer of large result sets but not so suitable for enterprise data warehousing. 


### A Word on Non-Relational Databases

Among other things, relational databases are ACID (Atomicity, Consistency, Isolation, and Durability) compliant and ask for very little in return to provide us with a framework to keep our data quality high for decades. So unless, you have a very specific use case that translates to a compelling reason to use a non-relational database -- stick to SQL. Document oriented storage or very unstructured information could be such a reason to use a non-relational database, yet their JSON support allow to also handle JSON in database cells.  About a decade ago [mongoDB](https://www.mongodb.com/) gained traction, partly piggy-backing the success of Javascript and serverside javascript in particular. In web development the MEAN (mongoDB, expressjs, angular and node) stack become popular and with the bundle the idea of non-relational databases as fast track to a backend spread. 

Columnar stores which are also considered non-relational are conceptional very similar to relational databases though denormalized and designed to structure sparse data. Database systems like [Apache Cassandra](https://cassandra.apache.org/) are designed to scale horizontally and be highly available managing massive amounts of data. Cloud applications that distribute data across multiple nodes for high availability benefit from such an approach. Other options include [Redis](https://redis.io/) or [Couchbase](https://www.couchbase.com/).
If your are not happy with the 'beyond-the-scope-of-this-book' argument, blogging experts like [Lukas Eder](https://blog.jooq.org/tag/nosql/) maybe biased but much better educated (and fun) to educate you here.


## Non Technical Aspects of Managing Data

The fact that we can do more data work single handedly than ever before does not only equate to more options. It also means we need to be aware of new issues and responsibilities.


### Etiquette

For example, the ability to scrape a website daily and doing so with good intent for the sake of science does not mean a website's AUP (Accetable Use Policy) allows to systematically archive its content. 

>Be responsible when scraping data from websites by following polite principles: introduce yourself, ask for permission, take slowly and never ask twice. --CRAN description of the polite R package.

In other words, the new type of researcher discussed in this book needs awareness for potential legal consequences and online community etiquette, and ultimately the ability to choose an approach. The {polite} R package quoted above is an example for an alternative approach that favors etiqutte over hiding IP addresses to avoid access denial. 


### Privacy 

- aggreagtion level, assess the sensitivity of data, anonymize data



### Security

- do not store passwords
- tokens, RSA Keys 
- access control easier via databases. 


### Open Data 

- publicly funded data 











